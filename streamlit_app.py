import streamlit as st
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RandomizedSearchCV, train_test_split, TimeSeriesSplit
import altair as alt
import plotly.express as px
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏£‡∏Å (Random Forest)
def load_data(file):
    message_placeholder = st.empty()  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô
    if file is None:
        st.error("‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV")
        return None
    
    try:
        df = pd.read_csv(file)
        if df.empty:
            st.error("‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            return None
        message_placeholder.success("‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")  # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á
        return df
    except pd.errors.EmptyDataError:
        st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ ‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≤‡∏à‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        return None
    except pd.errors.ParserError:
        st.error("‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå CSV ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå")
        return None
    except Exception as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {e}")
        return None
    finally:
        message_placeholder.empty()  # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå

# def fix_outliers_based_on_neighbors(data, threshold=0.5, decimal_threshold=0.01):
#     # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏ñ‡∏ß‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏ñ‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
#     data['avg_neighbors'] = (data['wl_up'].shift(1) + data['wl_up'].shift(-1)) / 2
    
#     # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
#     data['diff_avg'] = (data['wl_up'] - data['avg_neighbors']).abs()
    
#     # ‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô outlier ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏° threshold ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
#     large_outlier_condition = data['diff_avg'] > threshold
    
#     # ‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô outlier ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (decimal threshold)
#     small_outlier_condition = (data['diff_avg'] > decimal_threshold) & (data['diff_avg'] <= threshold)
    
#     # ‡∏£‡∏ß‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö outlier
#     outlier_condition = large_outlier_condition | small_outlier_condition
    
#     # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ NaN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ wl_up ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô outlier
#     data.loc[outlier_condition, 'wl_up'] = np.nan
    
#     # ‡πÉ‡∏ä‡πâ interpolation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NaN
#     data['wl_up'] = data['wl_up'].interpolate(method='linear')
    
#     # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
#     data.drop(columns=['avg_neighbors', 'diff_avg'], inplace=True)
    
#     return data

def clean_data(df):
    data_clean = df.copy()
    data_clean['datetime'] = pd.to_datetime(data_clean['datetime'], errors='coerce')
    data_clean = data_clean.dropna(subset=['datetime'])
    data_clean = data_clean[(data_clean['wl_up'] >= -100)]
    data_clean = data_clean[(data_clean['wl_up'] != 0) & (~data_clean['wl_up'].isna())]
    
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ outliers ‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
    # data_clean = fix_outliers_based_on_neighbors(data_clean)
    
    return data_clean

def create_time_features(data_clean):
    if not pd.api.types.is_datetime64_any_dtype(data_clean['datetime']):
        data_clean['datetime'] = pd.to_datetime(data_clean['datetime'], errors='coerce')

    data_clean['year'] = data_clean['datetime'].dt.year
    data_clean['month'] = data_clean['datetime'].dt.month
    data_clean['day'] = data_clean['datetime'].dt.day
    data_clean['hour'] = data_clean['datetime'].dt.hour
    data_clean['minute'] = data_clean['datetime'].dt.minute
    data_clean['day_of_week'] = data_clean['datetime'].dt.dayofweek
    data_clean['day_of_year'] = data_clean['datetime'].dt.dayofyear
    data_clean['week_of_year'] = data_clean['datetime'].dt.isocalendar().week
    data_clean['days_in_month'] = data_clean['datetime'].dt.days_in_month

    return data_clean

def prepare_features(data_clean):
    feature_cols = [
        'year', 'month', 'day', 'hour', 'minute',
        'day_of_week', 'day_of_year', 'week_of_year',
        'days_in_month', 'wl_up_prev'
    ]
    X = data_clean[feature_cols]
    y = data_clean['wl_up']
    return X, y

def train_and_evaluate_model(X, y, model_type='random_forest'):
    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å
    if model_type == 'random_forest':
        model = train_random_forest(X_train, y_train)
    elif model_type == 'linear_regression':
        model = train_linear_regression_model(X_train, y_train)
    else:
        st.error("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        return None

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if model is None:
        st.error("‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß")
        return None
    return model

def train_random_forest(X_train, y_train):
    param_distributions = {
        'n_estimators': [100, 200, 500],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2],
        'max_features': ['auto', 'sqrt'],
        'bootstrap': [True, False]
    }

    rf = RandomForestRegressor(random_state=42)

    tscv = TimeSeriesSplit(n_splits=5)
    random_search = RandomizedSearchCV(
        estimator=rf,
        param_distributions=param_distributions,
        n_iter=20,
        cv=tscv,
        n_jobs=-1,
        verbose=2,
        random_state=42,
        scoring='neg_mean_absolute_error'
    )
    random_search.fit(X_train, y_train)

    return random_search.best_estimator_

def train_linear_regression_model(X_train, y_train):
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏£‡∏Å
def generate_missing_dates(data):
    full_date_range = pd.date_range(start=data['datetime'].min(), end=data['datetime'].max(), freq='15T')
    all_dates = pd.DataFrame(full_date_range, columns=['datetime'])
    data_with_all_dates = pd.merge(all_dates, data, on='datetime', how='left')
    return data_with_all_dates

def fill_code_column(data):
    if 'code' in data.columns:
        data['code'] = data['code'].fillna(method='ffill').fillna(method='bfill')
    return data

def handle_missing_values_by_week(data_clean, start_date, end_date, model_type='random_forest'):
    feature_cols = ['year', 'month', 'day', 'hour', 'minute',
                    'day_of_week', 'day_of_year', 'week_of_year', 'days_in_month', 'wl_up_prev']

    data = data_clean.copy()

    # Convert start_date and end_date to datetime
    start_date = pd.to_datetime(start_date)
    end_date = pd.to_datetime(end_date) + pd.DateOffset(hours=23, minutes=45)

    # Filter data based on the datetime range
    data = data[(data['datetime'] >= start_date) & (data['datetime'] <= end_date)]

    # Generate all missing dates within the selected range
    data_with_all_dates = generate_missing_dates(data)
    data_with_all_dates.index = pd.to_datetime(data_with_all_dates['datetime'])
    data_missing = data_with_all_dates[data_with_all_dates['wl_up'].isnull()]
    data_not_missing = data_with_all_dates.dropna(subset=['wl_up'])

    # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤ missing ‡πÉ‡∏ô wl_up_prev
    if 'wl_up_prev' in data_with_all_dates.columns:
        data_with_all_dates['wl_up_prev'] = data_with_all_dates['wl_up_prev'].interpolate(method='linear')
    else:
        data_with_all_dates['wl_up_prev'] = data_with_all_dates['wl_up'].shift(1).interpolate(method='linear')

    if len(data_missing) == 0:
        st.write("No missing values to predict.")
        return data_with_all_dates

    # Train initial model with all available data
    X_train, y_train = prepare_features(data_not_missing)
    model = train_and_evaluate_model(X_train, y_train, model_type=model_type)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if model is None:
        st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        return data_with_all_dates

    # Fill missing values
    for idx, row in data_missing.iterrows():
        X_missing = row[feature_cols].values.reshape(1, -1)
        try:
            predicted_value = model.predict(X_missing)[0]
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå wl_forecast ‡πÅ‡∏•‡∏∞ timestamp
            data_with_all_dates.loc[idx, 'wl_forecast'] = predicted_value
            data_with_all_dates.loc[idx, 'timestamp'] = pd.Timestamp.now()
        except Exception as e:
            st.warning(f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß {idx} ‡πÑ‡∏î‡πâ: {e}")
            continue

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå wl_up2 ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏¥‡∏°
    data_with_all_dates['wl_up2'] = data_with_all_dates['wl_up'].combine_first(data_with_all_dates['wl_forecast'])

    data_with_all_dates.reset_index(drop=True, inplace=True)
    return data_with_all_dates

def delete_data_by_date_range(data, delete_start_date, delete_end_date):
    # Convert delete_start_date and delete_end_date to datetime
    delete_start_date = pd.to_datetime(delete_start_date)
    delete_end_date = pd.to_datetime(delete_end_date)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡∏≠‡∏á data ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    data_to_delete = data[(data['datetime'] >= delete_start_date) & (data['datetime'] <= delete_end_date)]

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏°‡∏µ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    if len(data_to_delete) == 0:
        st.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á {delete_start_date} ‡πÅ‡∏•‡∏∞ {delete_end_date}.")
    elif len(data_to_delete) > (0.3 * len(data)):  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏•‡∏ö‡πÄ‡∏Å‡∏¥‡∏ô 30% ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        st.warning("‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å")
    else:
        # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ wl_up ‡πÄ‡∏õ‡πá‡∏ô NaN
        data.loc[data_to_delete.index, 'wl_up'] = np.nan

    return data

def calculate_accuracy_metrics(original, filled):
    # ‡∏ú‡∏™‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° datetime
    merged_data = pd.merge(original[['datetime', 'wl_up']], filled[['datetime', 'wl_up2']], on='datetime')

    # ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN ‡∏≠‡∏≠‡∏Å
    merged_data = merged_data.dropna(subset=['wl_up', 'wl_up2'])

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    mse = mean_squared_error(merged_data['wl_up'], merged_data['wl_up2'])
    mae = mean_absolute_error(merged_data['wl_up'], merged_data['wl_up2'])
    r2 = r2_score(merged_data['wl_up'], merged_data['wl_up2'])

    st.header("‡∏ú‡∏•‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥", divider='gray')

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric(label="Mean Squared Error (MSE)", value=f"{mse:.4f}")
    with col2:
        st.metric(label="Mean Absolute Error (MAE)", value=f"{mae:.4f}")
    with col3:
        st.metric(label="R-squared (R¬≤)", value=f"{r2:.4f}")

def plot_results(data_before, data_filled, data_deleted, data_deleted_option=False):
    data_before_filled = pd.DataFrame({
        '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': data_before['datetime'],
        '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°': data_before['wl_up']
    })

    data_after_filled = pd.DataFrame({
        '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': data_filled['datetime'],
        '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤': data_filled['wl_up2']
    })

    # ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö
    if data_deleted_option:
        data_after_deleted = pd.DataFrame({
            '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': data_deleted['datetime'],
            '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö': data_deleted['wl_up']
        })
    else:
        data_after_deleted = None

    # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    combined_data = pd.merge(data_before_filled, data_after_filled, on='‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', how='outer')

    if data_after_deleted is not None:
        combined_data = pd.merge(combined_data, data_after_deleted, on='‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', how='outer')

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ y ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü
    y_columns = ['‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤', '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°']
    if data_after_deleted is not None:
        y_columns.append('‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö')

    # Plot ‡∏î‡πâ‡∏ß‡∏¢ Plotly
    fig = px.line(combined_data, x='‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', y=y_columns,
                  labels={'value': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)', 'variable': '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'},
                  color_discrete_sequence=px.colors.qualitative.Plotly)

    fig.update_layout(xaxis_title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", yaxis_title="‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)")

    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü
    st.header("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ", divider='gray')
    st.plotly_chart(fig, use_container_width=True)

    st.header("‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤", divider='gray')
    data_filled_selected = data_filled[['code', 'datetime', 'wl_up', 'wl_forecast', 'timestamp']]
    st.dataframe(data_filled_selected, use_container_width=True)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    merged_data = pd.merge(data_before[['datetime', 'wl_up']], data_filled[['datetime', 'wl_up2']], on='datetime')
    merged_data = merged_data.dropna(subset=['wl_up', 'wl_up2'])
    comparison_data = merged_data[merged_data['wl_up2'] != merged_data['wl_up']]

    if comparison_data.empty:
        st.header("‡∏ú‡∏•‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥", divider='gray')
        st.info("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö")
    else:
        calculate_accuracy_metrics(data_before, data_filled)

def plot_data_preview(df_pre, df2_pre, total_time_lag):
    data_pre1 = pd.DataFrame({
        'datetime': df_pre['datetime'],
        '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢': df_pre['wl_up']
    })

    if df2_pre is not None:
        data_pre2 = pd.DataFrame({
            'datetime': df2_pre['datetime'] + total_time_lag,  # ‡∏Ç‡∏¢‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
            '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤': df2_pre['wl_up']
        })
        combined_data_pre = pd.merge(data_pre1, data_pre2, on='datetime', how='outer')

        # Plot ‡∏î‡πâ‡∏ß‡∏¢ Plotly ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î color_discrete_sequence
        fig = px.line(
            combined_data_pre, 
            x='datetime', 
            y=['‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤'],
            labels={'value': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)', 'variable': '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'},
            title='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ',
            color_discrete_sequence=px.colors.qualitative.Plotly
        )

        fig.update_layout(
            xaxis_title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", 
            yaxis_title="‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)"
        )

        # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        st.plotly_chart(fig, use_container_width=True)

    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        fig = px.line(
            data_pre1, 
            x='datetime', 
            y='‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢',
            labels={'‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)'},
            title='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢',
            color_discrete_sequence=px.colors.qualitative.Plotly
        )

        fig.update_layout(
            xaxis_title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", 
            yaxis_title="‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)"
        )

        st.plotly_chart(fig, use_container_width=True)

def merge_data(df1, df2=None):
    if df2 is not None:
        merged_df = pd.merge(df1, df2[['datetime', 'wl_up']], on='datetime', how='left', suffixes=('', '_prev'))
    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ df2 ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'wl_up_prev' ‡∏à‡∏≤‡∏Å 'wl_up' ‡∏Ç‡∏≠‡∏á df1 (shifted by 1)
        df1['wl_up_prev'] = df1['wl_up'].shift(1)
        merged_df = df1.copy()
    return merged_df

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ Linear Regression ‡∏ó‡∏µ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤
def forecast_with_linear_regression_single(data, forecast_start_date, forecast_days):
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï 1-30 ‡∏ß‡∏±‡∏ô
    if forecast_days < 1 or forecast_days > 30:
        st.error("‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 1 ‡∏ñ‡∏∂‡∏á 30 ‡∏ß‡∏±‡∏ô")
        return pd.DataFrame()

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    training_data_end = forecast_start_date - pd.Timedelta(minutes=15)
    training_data_start = forecast_start_date - pd.Timedelta(days=forecast_days) + pd.Timedelta(minutes=15)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if training_data_start < data.index.min():
        st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠")
        return pd.DataFrame()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    training_data = data.loc[training_data_start:training_data_end].copy()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå lag
    lags = [1, 4, 96, 192]  # lag 15 ‡∏ô‡∏≤‡∏ó‡∏µ, 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á, 1 ‡∏ß‡∏±‡∏ô, 2 ‡∏ß‡∏±‡∏ô
    for lag in lags:
        training_data[f'lag_{lag}'] = training_data['wl_up'].shift(lag)

    # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤ NaN ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á y_train ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    training_data.fillna(method='ffill', inplace=True)
    training_data.dropna(inplace=True)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå lag
    if training_data.empty:
        st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå lag")
        return pd.DataFrame()

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
    feature_cols = [f'lag_{lag}' for lag in lags]
    X_train = training_data[feature_cols]
    y_train = training_data['wl_up']

    # ‡∏™‡πÄ‡∏Å‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Linear Regression
    model = LinearRegression()
    model.fit(X_train_scaled, y_train)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
    forecast_periods = forecast_days * 96  # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (96 ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ 15 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô)
    forecast_index = pd.date_range(start=forecast_start_date, periods=forecast_periods, freq='15T')
    forecasted_data = pd.DataFrame(index=forecast_index, columns=['wl_up'])

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
    combined_data = data.copy()

    # ‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤
    for idx in forecasted_data.index:
        lag_features = {}
        for lag in lags:
            lag_time = idx - pd.Timedelta(minutes=15 * lag)
            if lag_time in combined_data.index and not pd.isnull(combined_data.at[lag_time, 'wl_up']):
                lag_value = combined_data.at[lag_time, 'wl_up']
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ lag ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á y_train
                lag_value = y_train.mean()
            lag_features[f'lag_{lag}'] = lag_value

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
        X_pred = pd.DataFrame([lag_features])
        X_pred_scaled = scaler.transform(X_pred)

        # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ñ‡πà‡∏≤
        forecast_value = model.predict(X_pred_scaled)[0]

        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
        forecast_value = np.clip(forecast_value, combined_data['wl_up'].min(), combined_data['wl_up'].max())
        
        forecasted_data.at[idx, 'wl_up'] = forecast_value

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï 'combined_data' ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        combined_data.at[idx, 'wl_up'] = forecast_value

    return forecasted_data

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ Linear Regression ‡∏™‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
def forecast_with_linear_regression_two(data, upstream_data, forecast_start_date, forecast_days, delay_hours):
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï 1-30 ‡∏ß‡∏±‡∏ô
    if forecast_days < 1 or forecast_days > 30:
        st.error("‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 1 ‡∏ñ‡∏∂‡∏á 30 ‡∏ß‡∏±‡∏ô")
        return pd.DataFrame()

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å upstream_data
    if not upstream_data.empty:
        upstream_data = upstream_data.copy()
        if delay_hours > 0:
            upstream_data.index = upstream_data.index + pd.Timedelta(hours=delay_hours)

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    training_data_end = forecast_start_date - pd.Timedelta(minutes=15)
    training_data_start = forecast_start_date - pd.Timedelta(days=forecast_days) + pd.Timedelta(minutes=15)

    if training_data_start < data.index.min():
        st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠")
        return pd.DataFrame()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    training_data = data.loc[training_data_start:training_data_end].copy()
    if not upstream_data.empty:
        training_data = training_data.join(upstream_data[['wl_up']], rsuffix='_upstream')

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå lag
    lags = [1, 4, 96, 192]  # lag 15 ‡∏ô‡∏≤‡∏ó‡∏µ, 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á, 1 ‡∏ß‡∏±‡∏ô, 2 ‡∏ß‡∏±‡∏ô
    for lag in lags:
        training_data[f'lag_{lag}'] = training_data['wl_up'].shift(lag)
        if not upstream_data.empty:
            training_data[f'lag_{lag}_upstream'] = training_data['wl_up_upstream'].shift(lag)

    # ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤ NaN ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á y_train ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    training_data.fillna(method='ffill', inplace=True)
    training_data.dropna(inplace=True)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå lag
    if training_data.empty:
        st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå lag")
        return pd.DataFrame()

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
    if not upstream_data.empty:
        feature_cols = [f'lag_{lag}' for lag in lags] + [f'lag_{lag}_upstream' for lag in lags]
    else:
        feature_cols = [f'lag_{lag}' for lag in lags]

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    feature_cols = [col for col in feature_cols if col in training_data.columns]

    X_train = training_data[feature_cols]
    y_train = training_data['wl_up']

    # ‡∏™‡πÄ‡∏Å‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• Linear Regression
    model = LinearRegression()
    model.fit(X_train_scaled, y_train)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
    forecast_periods = forecast_days * 96  # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (96 ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ 15 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô)
    forecast_index = pd.date_range(start=forecast_start_date, periods=forecast_periods, freq='15T')
    forecasted_data = pd.DataFrame(index=forecast_index, columns=['wl_up'])

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
    combined_data = data.copy()
    if not upstream_data.empty:
        combined_upstream = upstream_data.copy()

    # ‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤
    for idx in forecasted_data.index:
        lag_features = {}
        for lag in lags:
            lag_time = idx - pd.Timedelta(minutes=15 * lag)
            # ‡∏Ñ‡πà‡∏≤ lag ‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏´‡∏•‡∏±‡∏Å
            if lag_time in combined_data.index and not pd.isnull(combined_data.at[lag_time, 'wl_up']):
                lag_value = combined_data.at[lag_time, 'wl_up']
            else:
                lag_value = y_train.mean()
            lag_features[f'lag_{lag}'] = lag_value

            # ‡∏Ñ‡πà‡∏≤ lag ‡∏Ç‡∏≠‡∏á upstream
            if not upstream_data.empty:
                if lag_time in combined_upstream.index and not pd.isnull(combined_upstream.at[lag_time, 'wl_up']):
                    lag_value_upstream = combined_upstream.at[lag_time, 'wl_up']
                else:
                    lag_value_upstream = y_train.mean()
                lag_features[f'lag_{lag}_upstream'] = lag_value_upstream

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
        X_pred = pd.DataFrame([lag_features])
        X_pred_scaled = scaler.transform(X_pred)

        # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ñ‡πà‡∏≤
        forecast_value = model.predict(X_pred_scaled)[0]

        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
        forecast_value = np.clip(forecast_value, combined_data['wl_up'].min(), combined_data['wl_up'].max())
        
        forecasted_data.at[idx, 'wl_up'] = forecast_value

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï 'combined_data' ‡πÅ‡∏•‡∏∞ 'combined_upstream' ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        combined_data.at[idx, 'wl_up'] = forecast_value
        if not upstream_data.empty:
            combined_upstream.at[idx, 'wl_up'] = lag_features.get(f'lag_{lag}_upstream', y_train.mean())

    return forecasted_data

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
def plot_data_combined_two_stations(data, forecasted=None, upstream_data=None, label='‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥'):
    fig = px.line(data, x=data.index, y='wl_up', title=f'‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ {label}', labels={'x': '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà', 'wl_up': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)'})
    fig.update_traces(connectgaps=False)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
    fig.add_scatter(x=data.index, y=data['wl_up'], mode='lines', name='‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á (‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå)', line=dict(color='blue'))
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ upstream (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    if upstream_data is not None:
        fig.add_scatter(x=upstream_data.index, y=upstream_data['wl_up'], mode='lines', name='‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á (‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ upstream)', line=dict(color='green'))

    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
    if forecasted is not None and not forecasted.empty:
        fig.add_scatter(x=forecasted.index, y=forecasted['wl_up'], mode='lines', name='‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå', line=dict(color='red'))
    
    fig.update_layout(xaxis_title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", yaxis_title="‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)")
    return fig


# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
def calculate_error_metrics(original, forecasted):
    # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô index ‡∏Ç‡∏≠‡∏á forecasted ‡πÄ‡∏õ‡πá‡∏ô 'datetime' ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà
    if forecasted.index.name != 'datetime':
        forecasted = forecasted.reset_index().rename(columns={'index': 'datetime'})

    # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
    merged = pd.merge(original[['datetime', 'wl_up']], forecasted[['datetime', 'wl_up']], on='datetime', how='inner', suffixes=('_actual', '_forecasted'))

    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ NaN
    merged = merged.dropna(subset=['wl_up_actual', 'wl_up_forecasted'])

    if merged.empty:
        st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥")
        return None, None, None

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì MAE ‡πÅ‡∏•‡∏∞ RMSE
    mae = mean_absolute_error(merged['wl_up_actual'], merged['wl_up_forecasted'])
    rmse = mean_squared_error(merged['wl_up_actual'], merged['wl_up_forecasted'], squared=False)

    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    actual_forecasted_data = merged[['datetime', 'wl_up_actual', 'wl_up_forecasted']].copy()
    actual_forecasted_data.rename(columns={'wl_up_actual': 'Actual', 'wl_up_forecasted': 'Forecasted'}, inplace=True)

    return mae, rmse, actual_forecasted_data

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
def create_comparison_table_streamlit(forecasted_data, actual_data):
    comparison_df = pd.DataFrame({
        'Datetime': actual_data['datetime'],
        '‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á': actual_data['Actual'],
        '‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå': actual_data['Forecasted']
    })
    return comparison_df

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
def plot_data_preview(df_pre, df2_pre, total_time_lag):
    data_pre1 = pd.DataFrame({
        'datetime': df_pre['datetime'],
        '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢': df_pre['wl_up']
    })

    if df2_pre is not None:
        data_pre2 = pd.DataFrame({
            'datetime': df2_pre['datetime'] + total_time_lag,  # ‡∏Ç‡∏¢‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
            '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤': df2_pre['wl_up']
        })
        combined_data_pre = pd.merge(data_pre1, data_pre2, on='datetime', how='outer')

        # Plot ‡∏î‡πâ‡∏ß‡∏¢ Plotly ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î color_discrete_sequence
        fig = px.line(
            combined_data_pre, 
            x='datetime', 
            y=['‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤'],
            labels={'value': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)', 'variable': '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'},
            title='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ',
            color_discrete_sequence=px.colors.qualitative.Plotly
        )

        fig.update_layout(
            xaxis_title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", 
            yaxis_title="‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)"
        )

        # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü
        st.plotly_chart(fig, use_container_width=True)

    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        fig = px.line(
            data_pre1, 
            x='datetime', 
            y='‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢',
            labels={'‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)'},
            title='‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢',
            color_discrete_sequence=px.colors.qualitative.Plotly
        )

        fig.update_layout(
            xaxis_title="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà", 
            yaxis_title="‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ (wl_up)"
        )

        st.plotly_chart(fig, use_container_width=True)

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏™‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ
def merge_data(df1, df2=None):
    if df2 is not None:
        merged_df = pd.merge(df1, df2[['datetime', 'wl_up']], on='datetime', how='left', suffixes=('', '_prev'))
    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ df2 ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'wl_up_prev' ‡∏à‡∏≤‡∏Å 'wl_up' ‡∏Ç‡∏≠‡∏á df1 (shifted by 1)
        df1['wl_up_prev'] = df1['wl_up'].shift(1)
        merged_df = df1.copy()
    return merged_df

# Streamlit UI
st.set_page_config(
    page_title="‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥",
    page_icon="üåä",
    layout="wide"
)

st.markdown("""
# ‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥

‡πÅ‡∏≠‡∏õ Streamlit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• **Linear Regression** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå, 
‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ
""")
st.markdown("---")

# Sidebar: Upload files and choose date ranges
with st.sidebar:

    st.sidebar.title("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå")
    model_choice = st.sidebar.radio("", ("Random Forest", "Linear Regression"),
        label_visibility="collapsed"  # ‡∏ã‡πà‡∏≠‡∏ô label visibility
    )

    st.sidebar.title("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    if model_choice == "Random Forest":
        with st.sidebar.expander("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Random Forest", expanded=False):
            use_second_file = st.checkbox("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á", value=False)
            
            # ‡∏™‡∏•‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå
            if use_second_file:
                uploaded_file2 = st.file_uploader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", type="csv", key="uploader2")
                uploaded_file = st.file_uploader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢", type="csv", key="uploader1")
            else:
                uploaded_file2 = None  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô None ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á
                uploaded_file = st.file_uploader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢", type="csv", key="uploader1")

            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á
            if use_second_file:
                time_lag_days = st.number_input("‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏ß‡∏±‡∏ô)", value=0, min_value=0)
                total_time_lag = pd.Timedelta(days=time_lag_days)
            else:
                total_time_lag = pd.Timedelta(days=0)

        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô sidebar
        with st.sidebar.expander("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", expanded=False):
            start_date = st.date_input("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô", value=pd.to_datetime("2024-05-01"))
            end_date = st.date_input("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î", value=pd.to_datetime("2024-05-31"))
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            delete_data_option = st.checkbox("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", value=False)

            if delete_data_option:
                # ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡πà‡∏≠‡∏á‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏¥‡πä‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                st.header("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
                delete_start_date = st.date_input("‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", value=start_date, key='delete_start')
                delete_start_time = st.time_input("‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô", value=pd.Timestamp("00:00:00").time(), key='delete_start_time')
                delete_end_date = st.date_input("‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", value=end_date, key='delete_end')
                delete_end_time = st.time_input("‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î", value=pd.Timestamp("23:45:00").time(), key='delete_end_time')

        process_button = st.button("‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•", type="primary")

    elif model_choice == "Linear Regression":
        with st.sidebar.expander("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Linear Regression", expanded=False):
            use_upstream = st.checkbox("‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á", value=False)

            if use_upstream:
                uploaded_up_file = st.file_uploader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", type="csv", key="uploader_up_lr")

            uploaded_fill_file = st.file_uploader("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢", type="csv", key="uploader_fill_lr")
            
            if use_upstream:
                delay_hours = st.number_input("‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ß‡∏•‡∏≤‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)", value=0, min_value=0)

        # ‡πÅ‡∏¢‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
        with st.sidebar.expander("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", expanded=False):
            training_start_date = st.date_input("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", value=pd.to_datetime("2024-05-01"), key='training_start_lr')
            training_start_time = st.time_input("‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", value=pd.Timestamp("00:00:00").time(), key='training_start_time_lr')
            training_end_date = st.date_input("‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", value=pd.to_datetime("2024-05-31"), key='training_end_lr')
            training_end_time = st.time_input("‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•", value=pd.Timestamp("23:45:00").time(), key='training_end_time_lr')

        with st.sidebar.expander("‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå", expanded=False):
            forecast_days = st.number_input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå", value=3, min_value=1, step=1)

        process_button2 = st.button("‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•", type="primary")

# Main content: Display results after file uploads and date selection
if model_choice == "Random Forest":
    # ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
    pass
elif model_choice == "Linear Regression":
    if uploaded_fill_file:
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        try:
            target_df = pd.read_csv(uploaded_fill_file)
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {e}")
            target_df = pd.DataFrame()

        if target_df.empty:
            st.error("‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        else:
            target_df = clean_data(target_df)
            if target_df.empty:
                st.error("‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠")
            else:
                target_df = generate_missing_dates(target_df)
                target_df['datetime'] = pd.to_datetime(target_df['datetime'], errors='coerce').dt.tz_localize(None)  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô timezone-naive
                target_df = create_time_features(target_df)
                target_df['wl_up_prev'] = target_df['wl_up'].shift(1)
                target_df['wl_up_prev'] = target_df['wl_up_prev'].interpolate(method='linear')

                # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ñ‡πâ‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ
                if use_upstream and uploaded_up_file:
                    try:
                        upstream_df = pd.read_csv(uploaded_up_file)
                    except Exception as e:
                        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏Ç‡πâ‡∏≤‡∏á‡∏ö‡∏ô: {e}")
                        upstream_df = pd.DataFrame()

                    if upstream_df.empty:
                        st.error("‡πÑ‡∏ü‡∏•‡πå CSV ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
                        upstream_df = pd.DataFrame()
                    else:
                        upstream_df = clean_data(upstream_df)
                        if upstream_df.empty:
                            st.error("‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠")
                            upstream_df = pd.DataFrame()
                        else:
                            upstream_df = generate_missing_dates(upstream_df)
                            upstream_df['datetime'] = pd.to_datetime(upstream_df['datetime'], errors='coerce').dt.tz_localize(None)  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô timezone-naive
                            upstream_df = create_time_features(upstream_df)
                            upstream_df['wl_up_prev'] = upstream_df['wl_up'].shift(1)
                            upstream_df['wl_up_prev'] = upstream_df['wl_up_prev'].interpolate(method='linear')
                else:
                    upstream_df = None

                # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ
                if use_upstream and upstream_df is not None:
                    # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô plot_data_preview ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ
                    plot_data_preview(target_df, upstream_df, pd.Timedelta(hours=delay_hours))
                else:
                    # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
                    plot_data_preview(target_df, None, pd.Timedelta(hours=0))

                if process_button2:
                    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå..."):
                        # ‡∏£‡∏ß‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á datetime ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå
                        training_start_datetime = pd.Timestamp.combine(training_start_date, training_start_time)
                        training_end_datetime = pd.Timestamp.combine(training_end_date, training_end_time)

                        # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
                        training_data = target_df[(target_df['datetime'] >= training_start_datetime) & (target_df['datetime'] <= training_end_datetime)].copy()

                        if training_data.empty:
                            st.error("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•")
                        else:
                            forecast_start_date_actual = training_end_datetime + pd.Timedelta(minutes=15)

                            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
                            forecast_end_date_actual = forecast_start_date_actual + pd.Timedelta(days=forecast_days)
                            max_datetime = target_df['datetime'].max()
                            if forecast_end_date_actual > max_datetime:
                                st.warning("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

                            if use_upstream and upstream_df is not None and not upstream_df.empty:
                                # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ Linear Regression (‡∏™‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ)
                                forecasted_data = forecast_with_linear_regression_two(
                                    data=target_df.set_index('datetime'),
                                    upstream_data=upstream_df.set_index('datetime'),
                                    forecast_start_date=forecast_start_date_actual,
                                    forecast_days=forecast_days,
                                    delay_hours=delay_hours
                                )
                            else:
                                # ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ Linear Regression (‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
                                forecasted_data = forecast_with_linear_regression_single(
                                    data=target_df.set_index('datetime'),
                                    forecast_start_date=forecast_start_date_actual,
                                    forecast_days=forecast_days
                                )

                            if not forecasted_data.empty:
                                st.header("‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå", divider='gray')
                                st.plotly_chart(plot_data_combined_two_stations(target_df.set_index('datetime'), forecasted_data, label='‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢'))

                                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
                                mae, rmse, actual_forecasted_data = calculate_error_metrics(
                                    original=target_df,
                                    forecasted=forecasted_data
                                )

                                if actual_forecasted_data is not None:
                                    st.header("‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö", divider='gray')
                                    comparison_table = create_comparison_table_streamlit(forecasted_data, actual_forecasted_data)
                                    st.dataframe(comparison_table, use_container_width=True)

                                    st.header("‡∏ú‡∏•‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥", divider='gray')
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        st.metric(label="Mean Absolute Error (MAE)", value=f"{mae:.2f}")
                                    with col2:
                                        st.metric(label="Root Mean Squared Error (RMSE)", value=f"{rmse:.2f}")
                                else:
                                    st.info("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ MAE ‡πÅ‡∏•‡∏∞ RMSE ‡πÑ‡∏î‡πâ")
                            else:
                                st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠")
    else:
        st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ Linear Regression")








